{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Demo\n",
    "## 使用 XGBoost 预测移动用户流失\n",
    "---\n",
    "## 内容\n",
    "1. [背景](#背景)\n",
    "2. [准备环境](#准备环境)\n",
    "3. [准备数据](#准备数据)\n",
    "4. [模型训练](#模型训练)\n",
    "5. [模型编译](#模型编译)\n",
    "6. [模型评估](#模型托管)\n",
    "7. [（可选）清理环境](#（可选）清理环境)\n",
    "---\n",
    "## 背景\n",
    "\n",
    "这个 Demo 改编自该博客：\n",
    "[AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/)\n",
    "\n",
    "失去客户对于任何业务来说都是代价高昂的。\n",
    "尽早识别不满意的客户可以让您有机会为他们提供挽留奖励。\n",
    "在这个通过使用机器学习自动识别不满意的客户，也称为客户流失预测。\n",
    "并且通过使用 Amazon SageMaker 功能来管理实验、训练和调试模型，并在部署后对其进行监控。\n",
    "---\n",
    "## 准备环境\n",
    "\n",
    "使用之前需要指定:\n",
    "- Demo 中需要用到的 Python 库\n",
    "- S3 存储桶和前缀用于存放训练数据和模型数据\n",
    "- IAM 角色允许训练和托管时可以访问到相应的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket\n",
    "current_time = datetime.datetime.now().strftime('%G%m%d-%H%M%S')\n",
    "bucket = 'sagemaker-bjs-ryanlao'\n",
    "prefix = 'demo/xgb/customer-churn-' + current_time\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "# role = 'arn:aws-cn:iam::855501529395:role/SAGEMAKER_RW_ALL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 准备数据\n",
    "\n",
    "移动运营商有历史记录来体现他们哪些客户最终流失，哪些客户继续使用该服务。\n",
    "我们可以使用这些历史信息来训练可以预测客户流失的机器学习模型。\n",
    "训练模型后，我们可以将任意客户的配置文件信息（与我们用于训练模型的配置文件信息相同）传递给模型，以便模型预测该客户是否会流失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get -y -qq install unzip\n",
    "!wget https://sagemaker-lab-zhy-ryanlao.s3.cn-northwest-1.amazonaws.com.cn/dataset/DKD2e_data_sets.zip\n",
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "churn = pd.read_csv('./Data sets/churn.txt')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照现代标准，它是一个相对较小的数据集，只有 3,333 条记录，其中每条记录使用 21 个属性来描述一个未知的美国移动运营商的客户档案。这些属性包括：\n",
    "\n",
    "- `State`: 美国的州缩写\n",
    "- `Account Length`: 帐户处于活动状态的天数\n",
    "- `Area Code`: 客户电话号码的三位数区号\n",
    "- `Phone`: 客户电话号码\n",
    "- `Int’l Plan`: 客户是否使用国际漫游(yes/no)\n",
    "- `VMail Plan`: 客户是否使用语音信箱(yes/no)\n",
    "- `VMail Message`: 每月平均语音邮件数量\n",
    "- `Day Mins`: 白天的呼叫时长\n",
    "- `Day Calls`: 白天的呼叫次数\n",
    "- `Day Charge`: 白天的呼叫费用\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: 晚上拨打电话的计费费用\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: 夜间拨打电话的计费费用\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: 国际通话的计费费用\n",
    "- `CustServ Calls`: 打客服电话的次数\n",
    "- `Churn?`: 客户是否流失(true/false)\n",
    "\n",
    "最后一个属性 `Churn?` 是目标属性，即我们希望机器学习模型预测的属性。\n",
    "由于目标属性是二进制的，因此我们的模型将执行二进制预测，也称为二进制分类（二分类）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用交叉表(crosstab)显示每个类别特征的频率\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column],\n",
    "                        columns='% observations',\n",
    "                        normalize='columns'))\n",
    "\n",
    "# 每个数值特征的直方图(hist)\n",
    "print(\"count:\\t总数量\\n\", \"mean:\\t均值\\n\", \"std:\\t标准差\\n\", \"min:\\t最小值\\n\", \"%:\\t较低百分数位，中间百分数位，较高百分数位\\n\", \"max:\\t最大值\")\n",
    "display(churn.describe())\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以观察到：\n",
    "- `State` 相当均匀的分布\n",
    "- `Phone` 均是唯一值，如果能解析出前缀可能会有一些价值，但是如果没有上下文关联的话，可以避免使用\n",
    "- 只有 14% 的客户流失，数据分类不太均衡，但并不极端\n",
    "- 大多数特征都非常好地呈现出高斯分布。\n",
    "- `VMail Message` 是一个明显的例外 (`Area Code` 看上去需要转化成非数字化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis = 1 for dropping cplumn index as well\n",
    "churn = churn.drop('Phone', axis=1)\n",
    "# Change 'Area Code' type to object\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来看看每个特征和我们的目标变量之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn?':\n",
    "        display(pd.crosstab(index=churn[column],\n",
    "                            columns=churn['Churn?'],\n",
    "                            normalize='columns'))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有趣的是，我们看到流失的客户出现的特征是:\n",
    "- 地理分布相当均匀\n",
    "- 开通国际漫游的客户更有可能会流失\n",
    "- 不太可能开启语音信箱\n",
    "- 在白天通话呈现出两种情况 (明显高于或者低于未流失客户的平均值)\n",
    "- 更多次的客服通话 (这也是比较合理的，可以预计遇到大量问题的客户可能更容易流失)\n",
    "\n",
    "此外也可以看到，流失的客户在白天通话时长和白天通话费用这两个特征上显示出非常相似的分布。这并不奇怪，因为通话时长和计费也是相关联的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过散布矩阵查看数据关联和规律\n",
    "display(churn.corr())\n",
    "pd.plotting.scatter_matrix(churn, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到几个特征本质上彼此之间具有 100% 的相关性。在某些机器学习算法中如果包含这些特征对可能会造成灾难性问题，而在另一些算法中，它只会引入少量冗余和偏差。现在先把如下的计费项移除，因为他们和通话时长高度相关：\n",
    "- 白天通话计费与白天通话时长相对\n",
    "- 晚上通话计费与晚上通话时长相对\n",
    "- 夜间通话计费与夜间通话时长相对\n",
    "- 国际漫游计费和国际漫游时长相对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge',\n",
    "                    'Eve Charge',\n",
    "                    'Night Charge',\n",
    "                    'Intl Charge'],\n",
    "                   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规整了数据集，现在要确定使用哪种算法。\n",
    "如上所述，似乎有一些变量，其中高值和低值（但不是中间值）都可以预测流失。\n",
    "通过使用Amazon SageMaker提供的XGBoost算法容器，可以在托管的分布式设置中进行训练，然后将其作为实时预测终端进行托管。\n",
    "XGBoost使用渐变增强树，这些树自然地考虑到各个特征和目标特征之间的非线性关系，以及各特征之间的复杂交互。\n",
    "\n",
    "XGBoost 可以对 CSV 和 LibSVN 格式的数据进行训练，当前Demo环境的数据格式为CSV:\n",
    "- 第一列为预测特征\n",
    "- 没有标题行\n",
    "\n",
    "先将类别特征转换为数值特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode as one hot\n",
    "model_data = pd.get_dummies(churn)\n",
    "\n",
    "# concat 可以将数据根据不同的轴作简单的融合\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)],\n",
    "                       axis=1)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据拆分为训练、验证和测试集。这将有助于防止模型的过拟合和对未看到的数据测试模型的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729),\n",
    "                                                  [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把文件上传到 S3 里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 模型训练\n",
    "\n",
    "开始进行模型训练，指定 XGBoost 算法容器的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，因为使用 CSV 文件格式进行训练，所以需要创建 s3_input 并且在训练函数中指向 S3 中文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后指定 XGBoost 用到的一些超参数，举例如下：\n",
    "\n",
    "- `max_depth` 控制算法中每个树的构建深度。更深的树可以更好地拟合，但计算费用更高，并可能导致过度拟合。通常需要权衡大量少层树和少量深层树对模型性能的影响。\n",
    "- `subsample` 控制训练数据的采样。此超参数有助于减少过度拟合，但将其设置得太低也会导致数据模型耗尽。\n",
    "- `num_round` 控制提升回合的数量。此值指定随后使用先前迭代的残差训练的模型。同样，更多的回合应该能够更好地拟合训练数据，但是计算成本昂贵或导致过度拟合。\n",
    "- `eta` 控制每一轮提升的积极性。较大的值会导致更保守的提升。\n",
    "- `gamma` 控制树生长的密集程度。值越大，模型就越保守。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Use Spot instance for cost optimization\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.large',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    train_use_spot_instances=True,\n",
    "                                    train_max_wait=3600,\n",
    "                                    train_max_run=1800,\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 模型编译\n",
    "[Amazon SageMaker Neo](https://aws.amazon.com/sagemaker/neo/) \n",
    "- 可以对模型进行优化，使其运行速度高达两倍，同时不会损失精度。\n",
    "- 使用 `compile_model()` 功能，指定目标实例类型（比如m5）以及存储编译模型的 S3 存储桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = xgb\n",
    "try:\n",
    "    xgb.create_model()._neo_image_account(boto3.Session().region_name)\n",
    "except:\n",
    "    print('Neo is not currently supported in', boto3.Session().region_name)\n",
    "else:\n",
    "    output_path = '/'.join(xgb.output_path.split('/')[:-1])\n",
    "    compiled_model = xgb.compile_model(target_instance_family='ml_m5',\n",
    "                                       input_shape={'data':[1, 69]},\n",
    "                                       role=role,\n",
    "                                       framework='xgboost',\n",
    "                                       framework_version='0.7',\n",
    "                                       output_path=output_path)\n",
    "    compiled_model.name = 'deployed-demo-xgboost-customer-churn-' + current_time\n",
    "    compiled_model.image = get_image_uri(sess.boto_region_name, 'xgboost-neo', repo_version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 模型托管\n",
    "\n",
    "通过使用指定好的算法，现在创建一个模型并部署到托管终端节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = compiled_model.deploy(initial_instance_count = 1,\n",
    "                                      instance_type = 'ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "运行托管终端节点后，可以非常方便地从模型中进行实时预测，只需发出 HTTP POST 请求即可。\n",
    "设置序列化和反序列化来将 `test_data` NumPy 数组传递给终端节点后面的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用一个简单的功能进行：\n",
    "1. 遍历测试数据集\n",
    "2. 将其拆分小批次的行\n",
    "3. 将这些小批次行转换为 CSV 字符串有效负载\n",
    "4. 通过调用 XGBoost 终端节点获取小批次的预测\n",
    "5. 收集预测并从模型提供的 CSV 输出转换为 NumPy 数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.to_numpy()[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有很多方法可以比较机器学习模型的性能，首先先简单地将实际值与预测值进行比较。\n",
    "在这个 Demo 中，只是简单地预测客户是否流失（`1`或者`0`），它会产生一个简单的混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_由于算法的随机元素，结果可能会略有不同。_\n",
    "\n",
    "在这些流失者中:\n",
    "- 绝大部分我们做了正确地预测（True Positive & True Negative）\n",
    "- 有个别错误地预测了客户会流失，然后最终并没有流失（False Negative）\n",
    "- 还有个别预测了客户不会流失，然后最终却流失了 (False Positive)\n",
    "\n",
    "由于上面的 `np.round()` 函数使用的是一个简单的阈值（临界值）0.5。\n",
    "从 `xgboost` 的预测出现了0和1之间的连续值，使得它们变成二分类的状况。\n",
    "但是，由于客户流失的预计会给公司带来更多的费用，而不是主动地试图留住我们认为可能流失的客户，因此我们应考虑调整此阈值(临界值)。\n",
    "这几乎肯定会增加误报的数量，但也可以预期会增加真的正面预测的数量，减少误报的数量。\n",
    "\n",
    "为了在这里得到一个粗略的直觉，接下来看看预测的连续值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来自模型的连续值预测倾向于向 0 或 1 倾斜，但在 0.1 到 0.9 之间有足够多的取值。\n",
    "用来调整临界值确实会改变一些客户的预测。例如..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，将临界值从 0.5 更改为 0.3 会导致预测结果出现少量变化。\n",
    "这里的数字总体来说很少，但是由于临界值的变化，总体上只有 6-10% 的客户正在发生变化。\n",
    "这是正确的决定吗？我们最终可能会留住个别额外的客户，但我们也额外不必要地激励了本来会留下的客户。\n",
    "确定最佳临界点是在真实环境中正确应用机器学习的关键步骤。\n",
    "比如，寻求最佳的临界值以获得更高的投资回报率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （可选）清理环境\n",
    "\n",
    "如果已经完成了 Demo，可以运行下面的命令来删除创建的托管终端节点，并避免留下的实例产生的任何费用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}